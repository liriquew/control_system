{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"dataset/Questions.csv\", encoding=\"ISO-8859-1\")\n",
    "questions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_csv(\"dataset/Tags.csv\", encoding=\"ISO-8859-1\")\n",
    "print(f\"\"\"Unique Tags count: {tags[\"Tag\"].unique().shape[0]}\"\"\")\n",
    "tags.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Группировка и объединение тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[\"Tag\"] = tags[\"Tag\"].astype(str)\n",
    "grouped_tags = tags.groupby(\"Id\")[\"Tag\"].apply(lambda tags: \" \".join(tags))\n",
    "print(grouped_tags.shape)\n",
    "grouped_tags.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_tags = grouped_tags.reset_index(name=\"Tags\")\n",
    "df_grouped_tags.columns = [\"Id\", \"Tags\"]\n",
    "df_grouped_tags.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление ненужных колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.drop(columns=[\"OwnerUserId\", \"CreationDate\", \"ClosedDate\", \"Title\"], inplace=True)\n",
    "questions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение вопросов и тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = questions.merge(df_grouped_tags, on=\"Id\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация по показателю рейтинга (Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Minimum Score: {data[\"Score\"].min()}\n",
    "Maximum Score: {data['Score'].max()}\n",
    "\n",
    "Total count {data[\"Score\"].count()}\n",
    "Count (Score > 0) {data[data[\"Score\"] > 0][\"Score\"].count()}\n",
    "Count (Score > 5) {data[data[\"Score\"] > 5][\"Score\"].count()}\n",
    "Count (Score > 10) {data[data[\"Score\"] > 20][\"Score\"].count()}\n",
    "Count (Score > 20) {data[data[\"Score\"] > 10][\"Score\"].count()}\n",
    "\n",
    "Describe:\\n{data[\"Score\"].describe()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### График распределения рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "ax1.hist(data[\"Score\"], bins=100, color=\"green\", edgecolor=\"black\")\n",
    "ax1.set_title(\"Распределение всего рейтинга (Score)\")\n",
    "ax1.set_xlabel(\"Score\")\n",
    "ax1.set_ylabel(\"Количество вопросов\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "ax2.hist(data[data[\"Score\"] < 5][\"Score\"], bins=100, color=\"red\", edgecolor=\"black\")\n",
    "ax2.set_title(\"Распределение рейтинга (Score < 5)\")\n",
    "ax2.set_xlabel(\"Score\")\n",
    "ax2.set_ylabel(\"Количество вопросов\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление записей с низким показателем рейтига, удаление ненужных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"Score\"] > 5) & (data[\"Score\"] < 2000)]\n",
    "data.drop(columns=[\"Id\", \"Score\"], inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "print(data.info())\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка заголовка и описания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изначальный вид описания\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in data[\"Body\"].head(5).items():\n",
    "    print(f\"=== Запись {idx} ===\")\n",
    "    print(text[:200] + \"...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация зависимостей и настройка NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.data.path.append(\"/home/ql/nltk_data\") \n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"\"\"\n",
    "            FAILED (btw)\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конвейер обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    предобработка текста:\n",
    "    1. удаление HTML-тегов\n",
    "    2. расширение сокращений\n",
    "    3. удаление спецсимволов и цифр\n",
    "    4. лемматизация\n",
    "    5. удаление стоп-слов\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    text = contractions.fix(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    processed = [\n",
    "        lemmatizer.lemmatize(token, pos='v')\n",
    "        for token in tokens\n",
    "        if token not in stop_words and token not in punct\n",
    "    ]\n",
    "    return ' '.join(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=f\"Processing Body\")\n",
    "data[\"Body\"] = data[\"Body\"].progress_apply(preprocess_text)\n",
    "\n",
    "data = data.dropna(subset=[\"Body\", \"Tags\"], how=\"any\")\n",
    "print(data[data[\"Body\"].isnull()].count())\n",
    "print(data[data[\"Tags\"].isnull()].count())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "body_len = data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(body_len, bins=50, log_scale=True)\n",
    "plt.title(\"Распределение длины текста\")\n",
    "plt.xlabel(\"Длина текста (количество слов)\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.axvline(96, color=\"red\", linestyle=\"--\", label=f\"Среднее: 96\")\n",
    "plt.axvline(60, color=\"green\", linestyle=\"--\", label=f\"Медиана: 60\")\n",
    "plt.axvline(33, color=\"black\", linestyle=\"-\", label=f\"Квантиль 0.25: 33\")\n",
    "plt.axvline(110, color=\"black\", linestyle=\"-\", label=f\"Квантиль 0.75: 110\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(body_len.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len) >= 33) & (data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len) <= 110)]\n",
    "\n",
    "data[\"Body\"] = data[\"Body\"].fillna(\"\")\n",
    "\n",
    "filtered_data = data[\n",
    "    (data[\"Body\"].str.len().fillna(0) > 0)\n",
    "]\n",
    "\n",
    "data.dropna(subset=[\"Body\", \"Tags\"], how=\"any\")\n",
    "data = data[data[\"Body\"].apply(len) != 0]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in data[\"Body\"].head(5).items():\n",
    "    print(f\"=== Запись {idx} ===\")\n",
    "    print(text[:200] + \"...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tags\"] = data[\"Tags\"].apply(lambda x: x.split())\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_series = data[\"Tags\"].explode()\n",
    "\n",
    "unique_tags_count = flat_series.nunique()\n",
    "tag_counts = flat_series.value_counts()\n",
    "total_tags = flat_series.count()\n",
    "\n",
    "print(f\"Total tags: {total_tags}\")\n",
    "print(f\"Unique tags: {unique_tags_count}\")\n",
    "print(tag_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_TAGS_COUNT = 33\n",
    "keywords = FreqDist(flat_series)\n",
    "tags_features = [word[0] for word in keywords.most_common(COMMON_TAGS_COUNT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "labels, frequencies = zip(*keywords.most_common(COMMON_TAGS_COUNT))\n",
    "ax.bar(range(len(labels)), frequencies)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(20)) \n",
    "ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(5))\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('50 самых частых меток')\n",
    "plt.xlabel('Метки')\n",
    "plt.ylabel('Частота')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_features_set = set(tags_features)\n",
    "\n",
    "mask = data[\"Tags\"].apply(lambda tags: any(tag in tags_features_set for tag in tags))\n",
    "filtered_data = data[mask].copy()\n",
    "\n",
    "filtered_data[\"Tags\"] = filtered_data[\"Tags\"].apply(\n",
    "    lambda tags: [tag for tag in tags if tag in tags_features_set]\n",
    ")\n",
    "\n",
    "data = filtered_data[filtered_data[\"Tags\"].apply(len) > 0]\n",
    "\n",
    "print(f\"\"\"\n",
    "    Average tags in question: {filtered_data[\"Tags\"].apply(len).mean()}\n",
    "    Max tags in question: {filtered_data[\"Tags\"].apply(len).max()}\n",
    "\n",
    "    Average body length in question: {filtered_data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len).mean()}\n",
    "    Min body length in question: {filtered_data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len).min()}\n",
    "    Max body length in question: {filtered_data[\"Body\"].apply(lambda x: x.split(\" \")).apply(len).max()}\n",
    "\"\"\")\n",
    "\n",
    "data.to_csv(\"processed_dataset/processed_questions.csv\", index=False)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords.most_common(COMMON_TAGS_COUNT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, regularizers, metrics, layers, optimizers, callbacks\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_dataset/processed_questions.csv\", encoding=\"ISO-8859-1\",\n",
    "    dtype={\n",
    "        \"Body\": str\n",
    "    },\n",
    "    converters={\n",
    "        \"Tags\": lambda x: literal_eval(x)\n",
    "    }\n",
    ")\n",
    "\n",
    "data[\"Body\"] = data[\"Body\"].fillna(\"\")\n",
    "\n",
    "data = data[\n",
    "    data[\"Body\"].str.len().fillna(0) > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Body']\n",
    "Y = data['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES_BODY = 1280\n",
    "TOTAL_FEATURES = MAX_FEATURES_BODY\n",
    "\n",
    "vectorizer_X = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    min_df=150,\n",
    "    max_df = 1.0,\n",
    "    encoding = 'utf-8',\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"(?u)\\S\\S+\",\n",
    "    max_features=MAX_FEATURES_BODY,\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer_X.fit_transform(X)\n",
    "\n",
    "pickle.dump(vectorizer_X, open(\"vectorizers/BodyVectorizer.pickle\", \"wb\"))\n",
    "\n",
    "TOTAL_FEATURES = len(vectorizer_X.get_feature_names_out())\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_bin = multilabel_binarizer.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT_FRACTION = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size=TEST_SPLIT_FRACTION, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    metrics = [\n",
    "        ('loss', 'val_loss'),\n",
    "        ('auc', 'val_auc'),\n",
    "        ('recall', 'val_recall'),\n",
    "        ('precision', 'val_precision'),\n",
    "    ]\n",
    "    \n",
    "    for i, (metric, val_metric) in enumerate(metrics):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        \n",
    "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "        \n",
    "        if val_metric in history.history:\n",
    "            plt.plot(history.history[val_metric], label=f'Validation {metric}')\n",
    "        \n",
    "        plt.title(metric.upper())\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(history.history[\"recall\"], history.history[\"precision\"], label=\"Precision / recall\")\n",
    "    plt.title(\"Pecision / recall\")\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_model(model):\n",
    "    print(model.evaluate(X_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_TAGS_COUNT = 33\n",
    "EPOCHS_COUNT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import CategoricalFocalCrossentropy \n",
    "\n",
    "def balanced_class_weights(y, recall_factor=1.5, max_weight=5.0):\n",
    "    class_weights = {}\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_labels = y[:, class_idx]\n",
    "        pos_count = np.sum(class_labels)\n",
    "        neg_count = len(class_labels) - pos_count\n",
    "        \n",
    "        weight_positive = min(max_weight, \n",
    "                            (neg_count / (pos_count + 1e-6))**0.5 * recall_factor)\n",
    "        class_weights[class_idx] = weight_positive\n",
    " \n",
    "    return class_weights\n",
    "\n",
    "class_weights_dict = balanced_class_weights(y_train, recall_factor=1.4, max_weight=8.0)\n",
    "\n",
    "def build_model():\n",
    "    inputs = tf.keras.Input(shape=(TOTAL_FEATURES,))\n",
    "    \n",
    "    x = layers.Dense(768, activation='swish', kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    residual = layers.Dense(384, activation='swish', kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01))(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(384, activation='swish', kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Dense(256, activation='swish', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1_l2(0.001))(x)\n",
    "\n",
    "    outputs = layers.Dense(COMMON_TAGS_COUNT, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = optimizers.AdamW(\n",
    "        learning_rate=1e-4,\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=CategoricalFocalCrossentropy(\n",
    "            alpha=0.65,\n",
    "            gamma=1.4,\n",
    "            from_logits=False,\n",
    "        ),\n",
    "        metrics=[\n",
    "            metrics.Recall(name='recall', thresholds=0.4),\n",
    "            metrics.Precision(name='precision', thresholds=0.4),\n",
    "            metrics.AUC(name='auc'),\n",
    "            metrics.F1Score(name='f1', threshold=0.4)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weights_dict,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "model.save(\"fit_history/model_11.keras\")\n",
    "np.save(\"fit_history/hist_11\", history)\n",
    "\n",
    "check_model(model)\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fit_history/model_11.keras\")\n",
    "\n",
    "for layer in model.layers[:8]:\n",
    "    layer.trainable = False\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weights_dict,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "model.save(\"fit_history/model_11_1.keras\")\n",
    "np.save(\"fit_history/hist_11_1\", history)\n",
    "\n",
    "check_model(model)\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_combined_history(hist_path1, hist_path2):\n",
    "    hist1 = np.load(hist_path1, allow_pickle=True).item()\n",
    "    hist2 = np.load(hist_path2, allow_pickle=True).item()\n",
    "    \n",
    "    combined_hist = {}\n",
    "    for key in hist1.history.keys():\n",
    "        if key in hist2.history:\n",
    "            combined_hist[key] = np.concatenate((hist1.history[key], hist2.history[key]), axis=None)\n",
    "    \n",
    "    class CombinedHistory:\n",
    "        def __init__(self, history):\n",
    "            self.history = history\n",
    "    \n",
    "    return CombinedHistory(combined_hist)\n",
    "\n",
    "combined_history = load_combined_history(\n",
    "    'fit_history/hist_11.npy',\n",
    "    'fit_history/hist_11_1.npy'\n",
    ")\n",
    "\n",
    "plot_training_history(combined_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"processed_dataset/processed_questions.csv\", encoding=\"ISO-8859-1\", converters={\n",
    "        \"Tags\": lambda x: literal_eval(x)\n",
    "    })\n",
    "\n",
    "flat_series = data['Tags'].explode()\n",
    "\n",
    "keywords = FreqDist(flat_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_TAGS_COUNT = 33\n",
    "\n",
    "for i in range(7):\n",
    "    for tag, count in keywords.most_common(COMMON_TAGS_COUNT)[i*5:(i+1)*5]:\n",
    "        print(f\"{tag}: {count}\", end=\"\\t\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "vectorizer_X1 = joblib.load(\"vectorizers/BodyVectorizer.pickle\")\n",
    "multilabel_binarizer = joblib.load(\"vectorizers/TagsVectorizer.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top_ten(arr: list[float]):\n",
    "    h = []\n",
    "    for i, val in enumerate(arr):\n",
    "        h.append((-val, [i, val]))\n",
    "    heapq.heapify(h)\n",
    "\n",
    "    res = []\n",
    "    for i in range(10):\n",
    "        res.append(heapq.heappop(h)[1])\n",
    "\n",
    "    return res\n",
    "\n",
    "model = None\n",
    "\n",
    "def predict(body: str):\n",
    "    global model\n",
    "    X_input = vectorizer_X1.transform([body])\n",
    "    if model is None:\n",
    "        model = keras.models.load_model(\"fit_history/model_11_1.keras\")\n",
    "\n",
    "\n",
    "    probas = model.predict(X_input)\n",
    "\n",
    "    print(\"Predicted tags:\", multilabel_binarizer.inverse_transform((probas > 0.10).astype(int)))\n",
    "    print(\"Predicted tags (top 10):\", multilabel_binarizer.classes_[[t for t, _ in get_top_ten(probas[0])]])\n",
    "\n",
    "    # plt.plot(probas[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"    \n",
    "Create a button, which will use jQuery javascript script\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"    \n",
    "Rewrite our python backend view, which calculate bonus amount\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "The query fetching customer orders (JOIN on `customers`, `orders`, `products`) takes 15+ seconds.  \n",
    "- Analyze the execution plan with `EXPLAIN ANALYZE`.  \n",
    "- Add missing indexes (suggest candidates: `orders.customer_id`, `products.sku`).  \n",
    "- Rewrite the query to avoid correlated subqueries.  \n",
    "- Partition the `orders` table by `order_date` (YYYY-MM).  \n",
    "- Validate speed improvement (target: <1s).  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Implementing a sorting algorithm for large datasets in C++ with multithreading support\n",
    "\"\"\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Using Git to resolve merge conflicts after rebasing a feature branch\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Parsing JSON string in Android app and displaying data in RecyclerView\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Designing a WPF UI with dynamic data binding and custom styles in Visual Studio\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без явного упоминания меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Database connection errors when fetching records for a web page\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Application crashes after prolonged use on mobile devices\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Version history conflicts during branch integration\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Design and implement a real-time dashboard that visualizes sensor data streams with dynamic filtering capabilities. \n",
    "Ensure the solution supports 10K+ concurrent connections, provides historical data overlays, \n",
    "and maintains sub-second latency during peak loads across both desktop and mobile browsers\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Diagnose and resolve random null reference exceptions occurring in production when users submit complex forms.\n",
    "The issue manifests only after 15+ form interactions and appears correlated with multi-step validation workflows.\n",
    "Provide hotfix with regression tests\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Eliminate OWASP Top 10 vulnerabilities across all public APIs.\n",
    "Implement strict input validation, rate limiting, JWT token rotation, and automated penetration testing.\n",
    "Address critical CSRF findings from recent audit\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Create CI/CD pipeline that executes static code analysis, runs test suites across multiple runtime versions,\n",
    "generates deployment artifacts, and promotes builds between environments based on git branch policies\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"\"\"\n",
    "Migrate our legacy financial reconciliation engine from end-of-life platforms to modern infrastructure\n",
    "without disrupting daily transaction processing. Re-implement custom rounding rules and currency conversion logic with atomicity guarantees.\n",
    "Include zero-downtime cutover strategy and automated consistency validation across terabyte-scale historical datasets\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
