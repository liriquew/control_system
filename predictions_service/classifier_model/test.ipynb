{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"dataset/Questions.csv\", encoding=\"ISO-8859-1\")\n",
    "questions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_csv(\"dataset/Tags.csv\", encoding=\"ISO-8859-1\")\n",
    "tags.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Группировка и объединение тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[\"Tag\"] = tags[\"Tag\"].astype(str)\n",
    "grouped_tags = tags.groupby(\"Id\")[\"Tag\"].apply(lambda tags: \" \".join(tags))\n",
    "grouped_tags.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_tags = grouped_tags.reset_index(name='Tags')\n",
    "df_grouped_tags.columns = ['Id', 'Tags']\n",
    "df_grouped_tags.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление ненужных колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
    "questions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение вопросов и тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = questions.merge(df_grouped_tags, on='Id')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ и работа с показателем рейтинга (Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum Score: {data['Score'].min()}\")\n",
    "print(f\"Maximum Score: {data['Score'].max()}\")\n",
    "\n",
    "print(f\"Total count {data[\"Score\"].count()}\")\n",
    "print(f\"Count (Score > 0) {data[data[\"Score\"] > 0][\"Score\"].count()}\")\n",
    "print(f\"Count (Score > 5) {data[data[\"Score\"] > 5][\"Score\"].count()}\")\n",
    "print(f\"Count (Score > 10) {data[data[\"Score\"] > 20][\"Score\"].count()}\")\n",
    "print(f\"Count (Score > 20) {data[data[\"Score\"] > 10][\"Score\"].count()}\")\n",
    "\n",
    "print(f\"Describe\\n{data[\"Score\"].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### График распределения рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# График 1: Все значения Score\n",
    "ax1.hist(data[\"Score\"], bins=100, color='green', edgecolor='black')\n",
    "ax1.set_title('Распределение всего рейтинга (Score)')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Количество вопросов')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# График 2: Только Score < 5\n",
    "ax2.hist(data[data['Score'] < 5][\"Score\"], bins=100, color='red', edgecolor='black')\n",
    "ax2.set_title('Распределение рейтинга (Score < 5)')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Количество вопросов')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Автоматическая настройка отступов между графиками\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление записей с низким показателем рейтига, удаление ненужных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Score'] > 5]\n",
    "data.drop(columns=['Id', 'Score'], inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка тегов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пребразование строки тегов в список тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tags'] = data['Tags'].apply(lambda x: x.split())\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление редких тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_series = data['Tags'].explode()\n",
    "\n",
    "unique_tags_count = flat_series.nunique()\n",
    "tag_counts = flat_series.value_counts()\n",
    "total_tags = flat_series.count()\n",
    "\n",
    "print(f\"Total tags: {total_tags}\")\n",
    "print(f\"Unique tags: {unique_tags_count}\")\n",
    "print(tag_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_TAGS_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = FreqDist(flat_series)\n",
    "tags_features = [word[0] for word in keywords.most_common(COMMON_TAGS_COUNT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "\n",
    "labels, frequencies = zip(*keywords.most_common(COMMON_TAGS_COUNT))\n",
    "ax.bar(range(len(labels)), frequencies)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(20)) \n",
    "ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(5))\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('Топ-100 самых частых тегов')\n",
    "plt.xlabel('Теги')\n",
    "plt.ylabel('Частота')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_features_set = set(tags_features)\n",
    "\n",
    "mask = data[\"Tags\"].apply(lambda tags: any(tag in tags_features_set for tag in tags))\n",
    "filtered_data = data[mask].copy()\n",
    "\n",
    "filtered_data[\"Tags\"] = filtered_data[\"Tags\"].apply(\n",
    "    lambda tags: [tag for tag in tags if tag in tags_features_set]\n",
    ")\n",
    "\n",
    "data = filtered_data[filtered_data[\"Tags\"].apply(len) > 0]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка заголовка и описания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изначальный вид описания\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in data[\"Body\"].head(5).items():\n",
    "    print(f\"=== Запись {idx} ===\")\n",
    "    print(text[:200] + \"...\")  # Первые 200 символов для краткости\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация зависимостей и настройка NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')       # Токенизатор\n",
    "nltk.download('stopwords')   # Стоп-слова\n",
    "nltk.download('wordnet')     # Лемматизатор\n",
    "nltk.download('punkt_tab')   # Таблицы для токенизации (требуется для некоторых версий NLTK)\n",
    "\n",
    "# Указываем путь для сохранения данных (если нужно)\n",
    "nltk.data.path.append(\"/home/ql/nltk_data\") \n",
    "\n",
    "# Проверяем загрузку\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"\"\"\n",
    "            FAILED (btw)\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конвейер обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Полная предобработка текста:\n",
    "    1. Удаление HTML-тегов\n",
    "    2. Расширение сокращений\n",
    "    3. Удаление спецсимволов и цифр\n",
    "    4. Лемматизация\n",
    "    5. Удаление стоп-слов\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    # HTML -> текст\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Расширение сокращений и очистка\n",
    "    text = contractions.fix(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # Удаляем всё кроме букв и пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Токенизация и лемматизация\n",
    "    tokens = word_tokenize(text)\n",
    "    processed = [\n",
    "        lemmatizer.lemmatize(token, pos='v')  # Сначала глаголы\n",
    "        for token in tokens\n",
    "        if token not in stop_words and token not in punct\n",
    "    ]\n",
    "    return ' '.join(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем обработку к Title и Body\n",
    "for column in ('Title', 'Body'):\n",
    "    tqdm.pandas(desc=f\"Processing {column}\")\n",
    "    data[column] = data[column].progress_apply(preprocess_text)\n",
    "\n",
    "data = data.dropna(subset=['Title', 'Body', 'Tags'], how='any')\n",
    "print(data[data[\"Body\"].isnull()].count())\n",
    "print(data[data[\"Title\"].isnull()].count())\n",
    "print(data[data[\"Tags\"].isnull()].count())\n",
    "\n",
    "\n",
    "# Сохраняем результат\n",
    "data.to_csv('processed_dataset/processed_questions.csv', index=False)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in data[\"Body\"].head(5).items():\n",
    "    print(f\"=== Запись {idx} ===\")\n",
    "    print(text[:200] + \"...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data[\"Title\"].explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, regularizers, metrics, layers, optimizers, callbacks\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_dataset/processed_questions.csv\", encoding=\"ISO-8859-1\",\n",
    "    dtype={\n",
    "        \"Title\": str,\n",
    "        \"Body\": str\n",
    "    },\n",
    "    converters={\n",
    "        \"Tags\": lambda x: literal_eval(x)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data['Body']\n",
    "X2 = data['Title']\n",
    "Y = data['Tags']\n",
    "\n",
    "# Заменяем NaN на пустые строки в X1 и X2\n",
    "X1 = X1.fillna('')\n",
    "X2 = X2.fillna('')\n",
    "\n",
    "# Убедимся, что все элементы являются строками\n",
    "X1 = X1.astype(str)\n",
    "X2 = X2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_bin = multilabel_binarizer.fit_transform(Y)\n",
    "\n",
    "pickle.dump(multilabel_binarizer, open(\"vectorizers/TagsVectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n",
    "                                       min_df=0.0,\n",
    "                                       max_df = 1.0,\n",
    "                                       strip_accents = None,\n",
    "                                       encoding = 'utf-8', \n",
    "                                       preprocessor=None,\n",
    "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
    "                                       max_features=1000)\n",
    "\n",
    "vectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n",
    "                                       min_df=0.0,\n",
    "                                       max_df = 1.0,\n",
    "                                       strip_accents = None,\n",
    "                                       encoding = 'utf-8', \n",
    "                                       preprocessor=None,\n",
    "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
    "                                       max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_tfidf = vectorizer_X1.fit_transform(X1)\n",
    "X2_tfidf = vectorizer_X2.fit_transform(X2)\n",
    "X_tfidf = hstack([X1_tfidf, X2_tfidf])\n",
    "\n",
    "pickle.dump(vectorizer_X1, open(\"vectorizers/BodyVectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(vectorizer_X2, open(\"vectorizers/TitleVectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT_FRACTION = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = TEST_SPLIT_FRACTION, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_TAGS_COUNT = 100\n",
    "EPOCHS_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupAwareRegularizer(regularizers.Regularizer):\n",
    "    def __init__(self, groups):\n",
    "        \"\"\"\n",
    "        groups: список словарей вида [\n",
    "            {'class_indices': [0,1,2], 'penalty': 0.3},  # Доминирующие классы\n",
    "            {'class_indices': [25,37], 'penalty': 0.1},   # Редкие классы\n",
    "            {'default_penalty': 0.05}                     # Остальные\n",
    "        ]\n",
    "        \"\"\"\n",
    "        self.groups = groups\n",
    "        self.penalty_map = self._build_penalty_map()\n",
    "        \n",
    "    def _build_penalty_map(self):\n",
    "        penalty_map = tf.ones(COMMON_TAGS_COUNT, dtype=tf.float32)\n",
    "        default = next((g['default_penalty'] for g in self.groups if 'default_penalty' in g), 0.0)\n",
    "        \n",
    "        for group in self.groups:\n",
    "            if 'class_indices' in group:\n",
    "                indices = tf.constant(group['class_indices'], dtype=tf.int32)\n",
    "                updates = tf.fill([len(indices)], group['penalty'])\n",
    "                penalty_map = tf.tensor_scatter_nd_update(\n",
    "                    penalty_map, \n",
    "                    tf.expand_dims(indices, -1), \n",
    "                    updates\n",
    "                )\n",
    "        penalty_map = tf.where(penalty_map == 0.0, default, penalty_map)\n",
    "        return penalty_map\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: тензор активаций формы (batch_size, COMMON_TAGS_COUNT)\n",
    "        \"\"\"\n",
    "        # Штрафуем активации пропорционально их величине и группе\n",
    "        penalties = self.penalty_map * tf.reduce_mean(tf.square(x), axis=0)\n",
    "        return tf.reduce_sum(penalties)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'groups': self.groups}\n",
    "\n",
    "def focal_loss_model():\n",
    "    inputs = keras.Input(shape=(2000,))\n",
    "    \n",
    "    # Feature extraction\n",
    "    x = layers.Dense(1024, activation='swish', \n",
    "                    kernel_regularizer=regularizers.l1_l2(0.001, 0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Bottleneck with skip-connection\n",
    "    x_skip = layers.Dense(512, activation='swish')(x)\n",
    "    x = layers.BatchNormalization()(x_skip)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Add()([x, x_skip])  # Residual connection\n",
    "    \n",
    "    # Decision block\n",
    "    x = layers.Dense(256, activation='swish')(x)\n",
    "    \n",
    "    outputs = layers.Dense(\n",
    "        COMMON_TAGS_COUNT, \n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=GroupAwareRegularizer(  # Изменили на kernel_regularizer!\n",
    "            groups=[\n",
    "                {\n",
    "                    'class_indices': [0,1],  # C#, Java, JavaScript, Android, Python\n",
    "                    'penalty': 0.3\n",
    "                },\n",
    "                {\n",
    "                    'class_indices': [2,3,4,5],   # Пример редких классов\n",
    "                    'penalty': 0.2                 # Меньший штраф\n",
    "                },\n",
    "                {\n",
    "                    'class_indices': [6,7,8,9,10,11,12,13,14],   # Пример редких классов\n",
    "                    'penalty': 0.1                 # Меньший штраф\n",
    "                },\n",
    "                {\n",
    "                    'default_penalty': 0.05          # Все остальные\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Focal Loss для дисбаланса\n",
    "    loss = keras.losses.BinaryFocalCrossentropy(gamma=2.0, alpha=0.25)\n",
    "    \n",
    "    # Настройка оптимизатора\n",
    "    optimizer = keras.optimizers.Nadam(\n",
    "        learning_rate=keras.optimizers.schedules.ExponentialDecay(\n",
    "            0.001, 1000, 0.9)\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.PrecisionAtRecall(0.5),\n",
    "            metrics.AUC(multi_label=True, name='auc'),\n",
    "            metrics.RecallAtPrecision(0.7),\n",
    "            metrics.Precision(name='precision'),\n",
    "            metrics.Recall(name='recall'),\n",
    "            metrics.F1Score(name=\"F1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "model = focal_loss_model()\n",
    "model.summary()\n",
    "\n",
    "sample_weights = np.sum(y_train, axis=1)\n",
    "\n",
    "# Нормализуем веса\n",
    "sample_weights = sample_weights / np.mean(sample_weights)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sample_weight=sample_weights,  # Передаем веса примеров\n",
    "    epochs=EPOCHS_COUNT,\n",
    "    callbacks=[EarlyStopping(patience=3)]\n",
    ")\n",
    "\n",
    "model.save(\"fit_history/model_1.keras\")\n",
    "np.save(\"fit_history/hist_1\", history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLearner(layers.Layer):\n",
    "    def __init__(self, num_tokens=16):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.tokenizer = layers.Dense(self.num_tokens, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, features)\n",
    "        attn_weights = self.tokenizer(inputs)  # (batch, num_tokens)\n",
    "        return tf.einsum('bf,bk->bkf', inputs, attn_weights)  # (batch, num_tokens, features)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'num_tokens': self.num_tokens}\n",
    "\n",
    "class GroupAwareRegularizer(regularizers.Regularizer):\n",
    "    def __init__(self, groups):\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __call__(self, weights):\n",
    "        penalty = 0.0\n",
    "        for group in self.groups:\n",
    "            class_weights = tf.gather(weights, group['class_indices'], axis=1)\n",
    "            penalty += group['penalty'] * tf.reduce_sum(tf.square(class_weights))\n",
    "        return penalty\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'groups': self.groups}\n",
    "\n",
    "\n",
    "def build_enhanced_model(input_dim=2000, num_classes=100):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # 1. Token Learning\n",
    "    tokens = TokenLearner(num_tokens=64)(inputs)\n",
    "    tokens = layers.GlobalAveragePooling1D()(tokens)\n",
    "    \n",
    "    # 2. Main Path\n",
    "    x = layers.Dense(1024, activation='gelu')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # 3. Gated Residual Connection\n",
    "    gate = layers.Dense(1024, activation='sigmoid')(tokens)\n",
    "    x = layers.Multiply()([x, gate])\n",
    "    \n",
    "    # 4. Enhanced Bottleneck\n",
    "    x = layers.Concatenate()([\n",
    "        layers.Dense(512, activation='gelu')(x),\n",
    "        layers.Dense(512, activation='gelu')(tokens)\n",
    "    ])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # 5. Output Layer with Group-aware Regularization\n",
    "    outputs = layers.Dense(\n",
    "        num_classes,\n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=GroupAwareRegularizer([\n",
    "            {'class_indices': list(range(3)), 'penalty': 0.1},\n",
    "            {'class_indices': list(range(3, 7)), 'penalty': 0.01},\n",
    "            {'class_indices': list(range(7, 11)), 'penalty': 0.001}\n",
    "        ])\n",
    "    )(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # 6. Custom Optimizer Configuration\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=1e-4,\n",
    "            decay_steps=1000\n",
    "        ),\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            metrics.PrecisionAtRecall(0.5),\n",
    "            metrics.AUC(multi_label=True, name='auc'),\n",
    "            metrics.RecallAtPrecision(0.7),\n",
    "            metrics.Precision(name='precision'),\n",
    "            metrics.Recall(name='recall'),\n",
    "            metrics.F1Score(name=\"F1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_enhanced_model()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS_COUNT)\n",
    "\n",
    "model.save(\"fit_history/model_2.keras\")\n",
    "np.save(\"fit_history/hist_2\", history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLearner(layers.Layer):\n",
    "    def __init__(self, num_tokens=16):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.tokenizer = layers.Dense(self.num_tokens, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, features)\n",
    "        attn_weights = self.tokenizer(inputs)  # (batch, num_tokens)\n",
    "        return tf.einsum('bf,bk->bkf', inputs, attn_weights)  # (batch, num_tokens, features)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'num_tokens': self.num_tokens}\n",
    "\n",
    "class GroupAwareRegularizer(regularizers.Regularizer):\n",
    "    def __init__(self, groups):\n",
    "        self.groups = groups\n",
    "        \n",
    "    def __call__(self, weights):\n",
    "        penalty = 0.0\n",
    "        for group in self.groups:\n",
    "            class_weights = tf.gather(weights, group['class_indices'], axis=1)\n",
    "            penalty += group['penalty'] * tf.reduce_sum(tf.square(class_weights))\n",
    "        return penalty\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'groups': self.groups}\n",
    "\n",
    "\n",
    "def build_enhanced_model(input_dim=2000, num_classes=100):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # 1. Token Learning\n",
    "    tokens = TokenLearner(num_tokens=64)(inputs)\n",
    "    tokens = layers.GlobalAveragePooling1D()(tokens)\n",
    "    \n",
    "    # 2. Main Path\n",
    "    x = layers.Dense(1024, activation='gelu')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # 3. Gated Residual Connection\n",
    "    gate = layers.Dense(1024, activation='sigmoid')(tokens)\n",
    "    x = layers.Multiply()([x, gate])\n",
    "    \n",
    "    # 4. Enhanced Bottleneck\n",
    "    x = layers.Concatenate()([\n",
    "        layers.Dense(512, activation='gelu')(x),\n",
    "        layers.Dense(512, activation='gelu')(tokens)\n",
    "    ])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # 5. Output Layer with Group-aware Regularization\n",
    "    outputs = layers.Dense(\n",
    "        num_classes,\n",
    "        activation='sigmoid',\n",
    "        kernel_regularizer=GroupAwareRegularizer([\n",
    "            {'class_indices': list(range(3)), 'penalty': 0.1},\n",
    "            {'class_indices': list(range(3, 7)), 'penalty': 0.01},\n",
    "            {'class_indices': list(range(7, 11)), 'penalty': 0.001}\n",
    "        ])\n",
    "    )(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # 6. Custom Optimizer Configuration\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=1e-4,\n",
    "            decay_steps=1000\n",
    "        ),\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            metrics.PrecisionAtRecall(0.5),\n",
    "            metrics.AUC(multi_label=True, name='auc'),\n",
    "            metrics.RecallAtPrecision(0.7),\n",
    "            metrics.Precision(name='precision'),\n",
    "            metrics.Recall(name='recall'),\n",
    "            metrics.F1Score(name=\"F1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_enhanced_model()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS_COUNT)\n",
    "\n",
    "model.save(\"fit_history/model_3.keras\")\n",
    "np.save(\"fit_history/hist_3\", history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_dynamic_dropout_model(input_dim=2000, num_classes=100):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # 1. Входные слои\n",
    "    x = layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(1e-3))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # 2. Промежуточные слои с проекцией для residual\n",
    "    residual = layers.Dense(512, activation='swish')(x)  # Проекция до нужной размерности\n",
    "    x = layers.Dense(768, activation='swish')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # 3. Bottleneck с согласованием размерностей\n",
    "    x = layers.Dense(512, activation='swish')(x)  # Приводим к размеру residual\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Правильный skip connection (обе ветви 512)\n",
    "    x = layers.Add()([x, residual])\n",
    "    \n",
    "    # 4. Выходной слой\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Кастомная loss с весами классов\n",
    "    def weighted_bce(y_true, y_pred):\n",
    "        class_counts = tf.reduce_sum(y_true, axis=1)\n",
    "        weights = (1. / (class_counts)) * tf.reduce_mean(class_counts)\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        return tf.reduce_mean(bce * weights)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=weighted_bce,\n",
    "        metrics=[\n",
    "            metrics.PrecisionAtRecall(0.5),\n",
    "            metrics.AUC(multi_label=True, name='auc'),\n",
    "            metrics.RecallAtPrecision(0.7),\n",
    "            metrics.Precision(name='precision'),\n",
    "            metrics.Recall(name='recall'),\n",
    "            metrics.F1Score(name=\"F1\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = balanced_dynamic_dropout_model()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS_COUNT)\n",
    "\n",
    "model.save(\"fit_history/model_4.keras\")\n",
    "np.save(\"fit_history/hist_4\", history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    #print(\"Accuracy score: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Recall score: {}\".format(recall_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n",
    "    print(\"Precision score: {}\".format(precision_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n",
    "    print(\"F1 score: {}\".format(f1_score(y_pred, y_test, average='weighted')))\n",
    "    #print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
    "    print(\"---\")  \n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "svc = LinearSVC()\n",
    "perceptron = Perceptron()\n",
    "pac = PassiveAggressiveClassifier()\n",
    "\n",
    "for classifier in [ sgd, lr, mn, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    clf_name = classifier.__class__.__name__\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'fit_history/{clf_name}_{timestamp}.pkl'\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "    \n",
    "    # Оценка и вывод результатов\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация графиков\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    try:\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['loss'], label='Training Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    # AUC\n",
    "    try:\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['auc'], label='AUC')\n",
    "        plt.title('AUC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    # Precision@Recall\n",
    "    try:\n",
    "  \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['precision_at_recall'], label='Precision@Recall=0.5')\n",
    "        plt.title('Precision@Recall=0.5')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    # Recall@Precision\n",
    "    try:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['recall_at_precision'], label='Recall@Precision=0.7')\n",
    "        plt.title('Recall@Precision=0.7')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_model(model):\n",
    "    print(model.evaluate(X_test , y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"fit_history/\"\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    history = np.load(f\"fit_history/hist_{i+1}.npy\", allow_pickle=True).item()\n",
    "    model = keras.models.load_model(f\"fit_history/model_{i+1}.keras\")\n",
    "    plot_training_history(history.history)\n",
    "    print(f\"fit_history/model_{i+1}.keras\")\n",
    "    check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"processed_dataset/processed_questions.csv\", encoding=\"ISO-8859-1\", converters={\n",
    "        \"Tags\": lambda x: literal_eval(x)\n",
    "    })\n",
    "\n",
    "flat_series = data['Tags'].explode()\n",
    "\n",
    "\n",
    "keywords = FreqDist(flat_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for tag, count in keywords.most_common(COMMON_TAGS_COUNT)[i*5:(i+1)*5]:\n",
    "        print(f\"{tag}: {count}\", end=\"\\t\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# Загрузка векторизаторов и бинаризатора\n",
    "vectorizer_X1 = joblib.load(\"vectorizers/BodyVectorizer.pickle\")\n",
    "vectorizer_X2 = joblib.load(\"vectorizers/TitleVectorizer.pickle\")\n",
    "multilabel_binarizer = joblib.load(\"vectorizers/TagsVectorizer.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = \"\"\"Make some buttons on frontend\"\"\"\n",
    "BODY = \"\"\"    \n",
    "Create a button, which will use jQuery javascript script\n",
    "\"\"\"\n",
    "\n",
    "TITLE = \"\"\"Fix user profile page\"\"\"\n",
    "BODY = \"\"\"    \n",
    "Rewrite our python untyped backend view, which calculate user bonus amount, maybe fix celery, maybe it broke crontab \n",
    "\"\"\"\n",
    "\n",
    "TITLE = \"\"\"Refactor Backend Integration for Data Export\"\"\"\n",
    "BODY = \"\"\"\n",
    "The current CSV export in PyQt5 blocks the UI thread.  \n",
    "- Move export logic to a QThread worker.  \n",
    "- Replace string concatenation with pandas DataFrame for CSV generation.  \n",
    "- Add error handling for invalid data (show QMessageBox on failure).  \n",
    "- Allow cancellation via a \"Stop Export\" button.  \"\"\"\n",
    "\n",
    "# TITLE = \"\"\"Optimize Slow Customer Orders Query\"\"\"\n",
    "# BODY = \"\"\"\n",
    "# The query fetching customer orders (JOIN on `customers`, `orders`, `products`) takes 15+ seconds.  \n",
    "# - Analyze the execution plan with `EXPLAIN ANALYZE`.  \n",
    "# - Add missing indexes (suggest candidates: `orders.customer_id`, `products.sku`).  \n",
    "# - Rewrite the query to avoid correlated subqueries.  \n",
    "# - Partition the `orders` table by `order_date` (YYYY-MM).  \n",
    "# - Validate speed improvement (target: <1s).  \n",
    "# \"\"\"\n",
    "\n",
    "# TITLE = \"\"\"Dynamically added jQuery elements not triggering click events\"\"\"\n",
    "# BODY = \"\"\"I'm using jQuery to add new buttons to a div with append(), but the click events don't work on the new elements. My code:\n",
    "# javascript\n",
    "# Copy\n",
    "\n",
    "# $('#container').append('<button class='btn'>Click me</button>');  \n",
    "# $('.btn').on('click', () => alert('Button clicked'));  \n",
    "\n",
    "# # Events work on initial buttons but not dynamically added ones. How can I fix this?\"\"\"\n",
    "\n",
    "# TITLE = \"\"\"RecyclerView not updating after adding new items in Android\"\"\"\n",
    "# BODY = \"\"\"\n",
    "# I have a RecyclerView adapter that updates a list of data. After calling adapter.add(newItem) and adapter.notifyDataSetChanged(), the UI doesn't refresh. \n",
    "# My code uses ListAdapter with DiffUtil. What am I missing? Is there a threading issue?\n",
    "# \"\"\"\n",
    "\n",
    "# TITLE = \"\"\"Fix Cross-Platform Layout Issues in Flutter App\"\"\"  \n",
    "# BODY = \"\"\"  \n",
    "# UI renders differently on iOS/Android devices (text overflow, alignment).  \n",
    "# - Replace hardcoded sizes with MediaQuery-based layout.  \n",
    "# - Implement platform-aware ThemeData (Cupertino/Material).  \n",
    "# - Add golden tests for critical screens.  \n",
    "# - Use Flex widgets instead of Row/Column nesting.  \n",
    "# - Verify font scaling (1.0-2.0) accessibility requirements.  \n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# TITLE = \"\"\"Secure API Endpoints Against SQL Injection\"\"\"  \n",
    "# BODY = \"\"\"  \n",
    "# Raw SQL queries in ASP.NET Core 6 controllers are vulnerable to injection.  \n",
    "# - Replace string concatenation with Entity Framework parameterization.  \n",
    "# - Add Dapper's `DynamicParameters` for complex queries.  \n",
    "# - Implement regex filter for suspicious characters in request params.  \n",
    "# - Create automated tests with SQLMAP test cases.  \n",
    "# - Add rate limiting (max 5 req/sec) to brute-force endpoints.  \n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_new = vectorizer_X1.transform([TITLE])\n",
    "X2_new = vectorizer_X2.transform([BODY])\n",
    "\n",
    "# Объединение и преобразование в плотный формат\n",
    "X_input = hstack([X1_new, X2_new]).toarray()  # (1, 2000)\n",
    "# Загрузка модели\n",
    "model = keras.models.load_model(\"fit_history/model_3.keras\")\n",
    "\n",
    "probas = model.predict(X_input)\n",
    "labels = multilabel_binarizer.inverse_transform((probas > 0.20).astype(int))\n",
    "\n",
    "\n",
    "print(\"Predicted tags:\", labels)\n",
    "\n",
    "# print(probas[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(probas[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
